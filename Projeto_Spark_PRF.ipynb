{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "333ce1b5",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color: black; border: 1px solid Black;  width: 85%; padding: 30px; margin:auto; text-align: center; border-radius: 10px 0; font-weight: bold; color: white;\">Estruturando os registros de acidentes da base de dados da PRF</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42d0f80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliotecas a serem utilizadas\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03388b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/14 23:09:53 WARN Utils: Your hostname, LAPTOP-EEVQJBOG resolves to a loopback address: 127.0.1.1; using 172.17.246.73 instead (on interface eth0)\n",
      "22/07/14 23:09:53 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/14 23:09:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .master('local')\n",
    "    .appName('Projeto_Spark_PRF')\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f93d3d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2017 = spark.read.csv('./PRF/datatran2017.csv', header=True, inferSchema=True, encoding='Latin1', sep=';')\n",
    "df_2018 = spark.read.csv('./PRF/datatran2018.csv', header=True, inferSchema=True, encoding='Latin1', sep=';')\n",
    "df_2019 = spark.read.csv('./PRF/datatran2019.csv', header=True, inferSchema=True, encoding='Latin1', sep=';')\n",
    "df_2020 = spark.read.csv('./PRF/datatran2020.csv', header=True, inferSchema=True, encoding='Latin1', sep=';')\n",
    "df_2021 = spark.read.csv('./PRF/datatran2021.csv', header=True, inferSchema=True, encoding='Latin1', sep=';')\n",
    "df_2022 = spark.read.csv('./PRF/datatran2022.csv', header=True, inferSchema=True, encoding='Latin1', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6db34b",
   "metadata": {},
   "source": [
    "<h3>Juntar os DataFrames em um único:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3be31027",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_2017.union(df_2018).union(df_2019).union(df_2020).union(df_2021).union(df_2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc975eb",
   "metadata": {},
   "source": [
    "<h3>Verificar se existe colunas com valores nulos:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cf794a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coluna:uop, Quantidade de nulos:11091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 591:===============================================>         (5 + 1) / 6]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "for coluna in df.columns:\n",
    "    if df.filter(df[coluna].isNull()).count() > 0:\n",
    "        print(f'Coluna:{coluna}, Quantidade de nulos:{df.filter(df[coluna].isNull()).count()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a682436d",
   "metadata": {},
   "source": [
    "<h3>Elimine as colunas desnecessárias e organize os seus tipos primitivos:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2ddb4e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: double (nullable = true)\n",
      " |-- data_inversa: timestamp (nullable = true)\n",
      " |-- dia_semana: string (nullable = true)\n",
      " |-- horario: timestamp (nullable = true)\n",
      " |-- uf: string (nullable = true)\n",
      " |-- br: string (nullable = true)\n",
      " |-- km: string (nullable = true)\n",
      " |-- municipio: string (nullable = true)\n",
      " |-- causa_acidente: string (nullable = true)\n",
      " |-- tipo_acidente: string (nullable = true)\n",
      " |-- classificacao_acidente: string (nullable = true)\n",
      " |-- fase_dia: string (nullable = true)\n",
      " |-- sentido_via: string (nullable = true)\n",
      " |-- condicao_metereologica: string (nullable = true)\n",
      " |-- tipo_pista: string (nullable = true)\n",
      " |-- tracado_via: string (nullable = true)\n",
      " |-- uso_solo: string (nullable = true)\n",
      " |-- pessoas: integer (nullable = true)\n",
      " |-- mortos: integer (nullable = true)\n",
      " |-- feridos_leves: integer (nullable = true)\n",
      " |-- feridos_graves: integer (nullable = true)\n",
      " |-- ilesos: integer (nullable = true)\n",
      " |-- ignorados: integer (nullable = true)\n",
      " |-- feridos: integer (nullable = true)\n",
      " |-- veiculos: integer (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      " |-- regional: string (nullable = true)\n",
      " |-- delegacia: string (nullable = true)\n",
      " |-- uop: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dac48087",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = df\n",
    "temp = temp.drop('dia_semana', 'horario', 'km', 'sentido_via', 'tipo_pista', 'tracado_via', 'uso_solo', 'latitude', 'longitude','regional','delegacia', 'uop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "24a689fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = temp.withColumn('id', col('id').cast(IntegerType())).withColumn('br', col('br').cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9612e435",
   "metadata": {},
   "source": [
    "<h3>Salve o DataFrame em um arquivo CSV:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e932436a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "temp.coalesce(1).write.format('com.databricks.spark.csv').mode('overwrite').save('./PRF/dados_prf[2017_2022]', header=True, delimiter=';', enconding='Latin1')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
